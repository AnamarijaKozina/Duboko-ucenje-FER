{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_shootout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN/kJZuR7Y3LkSXbmTmbqxK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnamarijaKozina/Duboko-ucenje-FER/blob/main/mnist_shootout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fSnQxpx-Rt75"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Random2DGaussian:\n",
        "  \"\"\"Random bivariate normal distribution sampler\n",
        "\n",
        "  Hardwired parameters:\n",
        "      d0min,d0max: horizontal range for the mean\n",
        "      d1min,d1max: vertical range for the mean\n",
        "      scalecov: controls the covariance range \n",
        "\n",
        "  Methods:\n",
        "      __init__: creates a new distribution\n",
        "\n",
        "      get_sample(n): samples n datapoints\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  d0min=0 \n",
        "  d0max=10\n",
        "  d1min=0 \n",
        "  d1max=10\n",
        "  scalecov=5\n",
        "  \n",
        "  def __init__(self):\n",
        "    dw0,dw1 = self.d0max-self.d0min, self.d1max-self.d1min\n",
        "    mean = (self.d0min,self.d1min)\n",
        "    mean += np.random.random_sample(2)*(dw0, dw1)\n",
        "    eigvals = np.random.random_sample(2)\n",
        "    eigvals *= (dw0/self.scalecov, dw1/self.scalecov)\n",
        "    eigvals **= 2\n",
        "    theta = np.random.random_sample()*np.pi*2\n",
        "    R = [[np.cos(theta), -np.sin(theta)], \n",
        "         [np.sin(theta), np.cos(theta)]]\n",
        "    Sigma = np.dot(np.dot(np.transpose(R), np.diag(eigvals)), R)\n",
        "    self.get_sample = lambda n: np.random.multivariate_normal(mean,Sigma,n)\n",
        "\n",
        "  \n",
        "def graph_surface(function, model, rect, offset=0.5, width=256, height=256):\n",
        "  \"\"\"Creates a surface plot (visualize with plt.show)\n",
        "\n",
        "  Arguments:\n",
        "    function: surface to be plotted\n",
        "    rect:     function domain provided as:\n",
        "              ([x_min,y_min], [x_max,y_max])\n",
        "    offset:   the level plotted as a contour plot\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "\n",
        "  lsw = np.linspace(rect[0][1], rect[1][1], width) \n",
        "  lsh = np.linspace(rect[0][0], rect[1][0], height)\n",
        "  xx0,xx1 = np.meshgrid(lsh, lsw)\n",
        "  grid = np.stack((xx0.flatten(),xx1.flatten()), axis=1)\n",
        "  \n",
        "\n",
        "  #get the values and reshape them\n",
        "  values=function(model,grid).reshape((width,height))\n",
        "  \n",
        "  # fix the range and offset\n",
        "  delta = offset if offset else 0\n",
        "  maxval=max(np.max(values)-delta, - (np.min(values)-delta))\n",
        "  \n",
        "  # draw the surface and the offset\n",
        "  plt.pcolormesh(xx0, xx1, values, \n",
        "     vmin=delta-maxval, vmax=delta+maxval)\n",
        "    \n",
        "  if offset != None:\n",
        "    plt.contour(xx0, xx1, values, colors='black', levels=[offset])\n",
        "\n",
        "def graph_data(X,Y_, Y, special=[]):\n",
        "  \"\"\"Creates a scatter plot (visualize with plt.show)\n",
        "\n",
        "  Arguments:\n",
        "      X:       datapoints\n",
        "      Y_:      groundtruth classification indices\n",
        "      Y:       predicted class indices\n",
        "      special: use this to emphasize some points\n",
        "\n",
        "  Returns:\n",
        "      None\n",
        "  \"\"\"\n",
        "  # colors of the datapoint markers\n",
        "  palette=([0.5,0.5,0.5], [1,1,1], [0.2,0.2,0.2])\n",
        "  colors = np.tile([0.0,0.0,0.0], (Y_.shape[0],1))\n",
        "  for i in range(len(palette)):\n",
        "    colors[Y_==i] = palette[i]\n",
        "\n",
        "  # sizes of the datapoint markers\n",
        "  sizes = np.repeat(20, len(Y_))\n",
        "  sizes[special] = 40\n",
        "  \n",
        "  # draw the correctly classified datapoints\n",
        "  good = (Y_==Y)\n",
        "  plt.scatter(X[good,0],X[good,1], c=colors[good], \n",
        "              s=sizes[good], marker='o', edgecolors='black')\n",
        "\n",
        "  # draw the incorrectly classified datapoints\n",
        "  bad = (Y_!=Y)\n",
        "  plt.scatter(X[bad,0],X[bad,1], c=colors[bad], \n",
        "              s=sizes[bad], marker='s', edgecolors='black')\n",
        "\n",
        "def class_to_onehot(Y):\n",
        "  Yoh=np.zeros((len(Y),max(Y)+1))\n",
        "  Yoh[range(len(Y)),Y] = 1\n",
        "  return Yoh\n",
        "\n",
        "def eval_perf_multi(Y, Y_):\n",
        "  pr = []\n",
        "  n = max(Y_)+1\n",
        "  M = np.bincount(n * Y_ + Y, minlength=n*n).reshape(n, n)\n",
        "  for i in range(n):\n",
        "    tp_i = M[i,i]\n",
        "    fn_i = np.sum(M[i,:]) - tp_i\n",
        "    fp_i = np.sum(M[:,i]) - tp_i\n",
        "    tn_i = np.sum(M) - fp_i - fn_i - tp_i\n",
        "    recall_i = tp_i / (tp_i + fn_i + 1e-15)\n",
        "    precision_i = tp_i / (tp_i + fp_i + 1e-15)\n",
        "    pr.append( (recall_i, precision_i) )\n",
        "  \n",
        "  accuracy = np.trace(M)/np.sum(M)\n",
        "  \n",
        "  return accuracy, pr, M\n",
        "\n",
        "def eval_AP(ranked_labels):\n",
        "  \"\"\"Recovers AP from ranked labels\"\"\"\n",
        "  \n",
        "  n = len(ranked_labels)\n",
        "  pos = sum(ranked_labels)\n",
        "  neg = n - pos\n",
        "  \n",
        "  tp = pos\n",
        "  tn = 0.\n",
        "  fn = 0.\n",
        "  fp = neg\n",
        "  \n",
        "  sumprec=0\n",
        "  #IPython.embed()\n",
        "  for x in ranked_labels:\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)    \n",
        "\n",
        "    if x:\n",
        "      sumprec += precision\n",
        "      \n",
        "    #print (x, tp,tn,fp,fn, precision, recall, sumprec)\n",
        "    #IPython.embed()\n",
        "\n",
        "    tp -= x\n",
        "    fn += x\n",
        "    fp -= not x\n",
        "    tn += not x\n",
        "\n",
        "  return sumprec/pos\n",
        "\n",
        "def sample_gmm_2d(ncomponents, nclasses, nsamples):\n",
        "  # create the distributions and groundtruth labels\n",
        "  Gs=[]\n",
        "  Ys=[]\n",
        "  for i in range(ncomponents):\n",
        "    Gs.append(Random2DGaussian())\n",
        "    Ys.append(np.random.randint(nclasses))\n",
        "\n",
        "  # sample the dataset\n",
        "  X = np.vstack([G.get_sample(nsamples) for G in Gs])\n",
        "  Y_= np.hstack([[Y]*nsamples for Y in Ys])\n",
        "  \n",
        "  return X,Y_"
      ],
      "metadata": {
        "id": "Hcd43avqVWFD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PTDeep(nn.Module):\n",
        "  def __init__(self, layer_sizes, activation = torch.relu): \n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = len(layer_sizes)-1\n",
        "\n",
        "    self.weights = nn.ParameterList([nn.Parameter(torch.randn(layer_sizes[i], layer_sizes[i+1]), requires_grad=True) for i in range(self.layers)]).cuda()\n",
        "    self.biases = nn.ParameterList([nn.Parameter(torch.randn(layer_sizes[i+1]), requires_grad=True) for i in range(self.layers)]).cuda()\n",
        "\n",
        "    self.activation = activation\n",
        "\n",
        "  def forward(self, X): # X: N x dim\n",
        "    for i in range(self.layers-1):\n",
        "      X = torch.mm(X, self.weights[i]) + self.biases[i]\n",
        "      X = self.activation(X)\n",
        "    X = torch.mm(X, self.weights[self.layers-1]) + self.biases[self.layers-1]\n",
        "    return torch.softmax(X, axis=1)\n",
        "\n",
        "  def get_loss(self, X, Yoh_, param_lambda=0.0): \n",
        " \n",
        "    Y_pred = self.forward(X).cuda()\n",
        "    reg = 0\n",
        "    if param_lambda>0:\n",
        "      reg = sum([torch.norm(w) for w in self.weights])/X.shape[0]\n",
        "    r1 = torch.log(Y_pred+1e-15)*Yoh_\n",
        "    r2 = torch.sum(r1, axis=1)\n",
        "    loss = - torch.mean(r2) + (reg * param_lambda)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "  def count_params(self):\n",
        "    for name, p in self.named_parameters():\n",
        "      print(f'Parametar {name} ima oblik {p.shape}')\n",
        "\n",
        "\n",
        "def train(model, X, Yoh_, param_niter, param_delta, param_lambda=0.0):\n",
        "\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = param_delta)\n",
        "  losses = []\n",
        "  for iter in range(param_niter):\n",
        "    optimizer.zero_grad()\n",
        "    loss = model.get_loss(X, Yoh_, param_lambda)\n",
        "    losses.append(loss)\n",
        "    if (iter+1)%500==0:\n",
        "      print(f\"Korak {iter+1}, gubitak = {loss}\")\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  print(\"Gubitak tijekom epoha:\")\n",
        "  losses = torch.tensor(losses).cpu()\n",
        "  plt.plot(losses)\n",
        "  plt.show()\n",
        "  \n",
        "def train_with_batches_and_evaluate(model, X, Yoh, x_val, y_val, param_niter, param_delta, batch_size=64, param_lambda=0.0, variable_learning_rate=False):\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = param_delta)\n",
        "  scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1-param_delta)\n",
        "  total_losses= []\n",
        "  best_acc = 0.0\n",
        "  best_model = None\n",
        "  best_iter = None\n",
        "  \n",
        "  for iter in range(param_niter):\n",
        "    perm = torch.randperm(X.shape[0])\n",
        "    x_batches = [x.cuda() for x in torch.split(X[perm], batch_size)]\n",
        "    y_batches = [x.cuda() for x in torch.split(Yoh[perm], batch_size)]\n",
        "    losses=[]\n",
        "    for i, (features, target) in enumerate(zip(x_batches, y_batches)):\n",
        "      optimizer.zero_grad()\n",
        "      loss = model.get_loss(features, target, param_lambda)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      losses.append(loss)\n",
        "    total_loss = sum(losses)/len(losses)\n",
        "    total_losses.append(total_loss)\n",
        "    if variable_learning_rate:\n",
        "      scheduler.step()\n",
        "    if (iter+1)%10==0:\n",
        "      print(f\"Korak {iter+1}, gubitak = {total_loss}\")\n",
        "    acc = performance(model, x_val, y_val, printing=False)\n",
        "    if acc>best_acc:\n",
        "      best_model=model\n",
        "      best_iter = iter\n",
        "      best_acc=acc\n",
        "\n",
        "  print(\"Gubitak tijekom epoha:\")\n",
        "  total_losses = torch.tensor(total_losses).cpu()\n",
        "  plt.plot(total_losses)\n",
        "  plt.show()\n",
        "  print(f\"Najbolji performans model ima nakon {best_iter}. epohe, kada je tocnost na validacijskom skupu {best_acc}.\")\n",
        "  return best_model\n",
        "\n",
        "def eval(model, X):\n",
        "  outputs = model.forward(X).cpu().detach().numpy() # N x C\n",
        "  return outputs"
      ],
      "metadata": {
        "id": "B3Ws_cFZVkT3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.rcsetup import validate_color\n",
        "dataset_root = ''  # change this to your preference\n",
        "mnist_train = torchvision.datasets.MNIST(dataset_root, train=True, download=True)\n",
        "mnist_test = torchvision.datasets.MNIST(dataset_root, train=False, download=True)\n",
        "\n",
        "\n",
        "x_train, y_train = mnist_train.data, mnist_train.targets\n",
        "x_test, y_test = mnist_test.data, mnist_test.targets\n",
        "x_train, x_test = x_train.float().div_(255.0), x_test.float().div_(255.0)\n",
        "x_train_v = x_train[:48000]\n",
        "y_train_v = y_train[:48000]\n",
        "x_val = x_train[48000:]\n",
        "y_val = y_train[48000:]"
      ],
      "metadata": {
        "id": "pNNfRJJMTO_9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.imshow(x_train[36], cmap = plt.get_cmap('gray'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "G_Rw1WviVNmH",
        "outputId": "a756c7a6-ec75-44dc-9ce1-deef8b6ea555"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f977224b810>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def performance(model, x, y, printing = True): #x and y must be torch.tensor\n",
        "  probs = eval(model, x)\n",
        "  y_pred = torch.argmax(torch.tensor(probs), axis=1) # matrica predvidenih razreda, 0/1\n",
        "  with torch.no_grad():\n",
        "    yoh = torch.tensor(class_to_onehot(y)).cuda()\n",
        "    loss = model.get_loss(x, yoh)\n",
        "    print(f\"Gubitak: {loss}\")\n",
        "\n",
        "  accuracy, pr, M = eval_perf_multi(y_pred, y)\n",
        "  if printing:\n",
        "    print(f\"\\nTocnost: {accuracy}\\nOdaziv i preciznost, redom za klase: {pr}\\nProsjecni odaziv i preciznost: {np.mean(pr, axis=0)}\\nMatrica konfuzije:\\n{M}\\n\")\n",
        "  return accuracy\n"
      ],
      "metadata": {
        "id": "xMSV2mhAELr1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zadatak1(x_train, y_train, yoh_train, x_test, y_test):\n",
        "  model = PTDeep([784,10], activation = torch.relu)\n",
        "  train(model, x_train, yoh_train, 3000, 0.1, param_lambda = 0.15) #Ponovite za različite iznose regularizacije.\n",
        "  print(\"Performans na skupu za treniranje:\")\n",
        "  performance(model, x_train, y_train)\n",
        "  print(\"Performans na skupu za testiranje:\")\n",
        "  performance(model, x_test, y_test)\n",
        "  w = model.weights[0].detach().cpu().numpy().T.reshape((-1, 28, 28))\n",
        "  for i, matrix in enumerate(w):\n",
        "    plt.figure(figsize=(1.5, 1.5))\n",
        "    plt.imshow(w[i], cmap = plt.get_cmap('gray'))"
      ],
      "metadata": {
        "id": "KG942lbdD6KV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zadatak2i3(x_train, y_train, yoh_train, x_test, y_test, param_lambda=0.0):\n",
        "  best = None\n",
        "  best_acc = 0.0\n",
        "  for i,w in enumerate([[784, 100, 10], [784, 100, 100, 10], [784, 100, 100, 100, 10]]): #Last model improves dramatically with number of epochs (loss plateaus multiple times)\n",
        "    print(f\"Model s konfiguracijom {w}:\")\n",
        "    model = PTDeep(w, activation = torch.relu)\n",
        "    lr = 1.5\n",
        "    epochs = 2500\n",
        "    if i==0:\n",
        "      epochs = 1500\n",
        "    \n",
        "    train(model, x_train, yoh_train, epochs, lr, param_lambda)\n",
        "    print(\"Performans na skupu za treniranje:\")\n",
        "    performance(model, x_train, y_train)\n",
        "    print(\"Performans na skupu za testiranje:\")\n",
        "    acc = performance(model, x_test, y_test)\n",
        "    if(acc>best_acc):\n",
        "      best = model\n",
        "\n",
        "  Y_pred = best.forward(x_train).detach()\n",
        "  logvals = torch.log(Y_pred+1e-15)*yoh_train\n",
        "  print(logvals.shape)\n",
        "  loss = -torch.sum(logvals, axis=1)\n",
        "  print(Y_pred[:6])\n",
        "  print(yoh_train[:6])\n",
        "  print(loss[:6])\n",
        "  _, inds = torch.topk(loss, 5)\n",
        "  for i in inds:\n",
        "    plt.figure(figsize=(2,2))\n",
        "    plt.imshow(x_train[i].cpu().numpy().reshape((28,28)))"
      ],
      "metadata": {
        "id": "ntxgIK8vGQCv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "\n",
        "def train_mb(x_train, y_train, yoh_train, x_test, y_test, x_val, y_val, variable_learning_rate=False):  #zadatak 4, 5, 6\n",
        "  model = PTDeep([784,50,10], activation = torch.relu)\n",
        "  lr = 0.01\n",
        "  epochs = 40\n",
        "  model = train_with_batches_and_evaluate(model, x_train, yoh_train, x_val, y_val, epochs, lr, variable_learning_rate=variable_learning_rate)\n",
        "  performance(model,x_test, y_test)"
      ],
      "metadata": {
        "id": "PmWBPwQTqmS2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zadatak8(x_test, y_test):\n",
        "  model = PTDeep([784,10])\n",
        "  print(\"Performans slučajno incijaliziranog modela (koji nije vidio podatke za učenje):\")\n",
        "  performance(model, x_test, y_test)"
      ],
      "metadata": {
        "id": "UZFXV-Lvj5z2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "def zadatak9(x_train, y_train, x_test, y_test):\n",
        "  #data needs to be numpy\n",
        "  x_train = x_train.detach().cpu().numpy()\n",
        "  x_test = x_test.detach().cpu().numpy()\n",
        "  y_train = y_train.detach().numpy()\n",
        "  y_test = y_test.detach().numpy()\n",
        "  print(\"Torch -> Numpy done.\")\n",
        "  classifier = svm.SVC(kernel='linear', decision_function_shape='ovo')\n",
        "  print(\"Classifier created. Training...\")\n",
        "  classifier.fit(x_train, y_train)\n",
        "  print(\"Training finished!\")\n",
        "  y_pred = classifier.predict(x_test)\n",
        "  print(\"Predictions made.\")\n",
        "  accuracy, pr, M = eval_perf_multi(y_pred, y_test)\n",
        "  print(f\"\\nTocnost: {accuracy}\\nOdaziv i preciznost, redom za klase: {pr}\\nProsjecni odaziv i preciznost: {np.mean(pr, axis=0)}\\nMatrica konfuzije:\\n{M}\\n\")"
      ],
      "metadata": {
        "id": "Qje5xb3OkHhU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  np.random.seed(100)\n",
        "\n",
        "  yoh_train = torch.tensor(class_to_onehot(y_train)).cuda()\n",
        "  x_train = torch.tensor(x_train.reshape((-1,28*28))).cuda()\n",
        "  y_train = torch.tensor(y_train)\n",
        "  x_train_v = torch.tensor(x_train_v.reshape((-1,28*28))).cuda()\n",
        "  y_train_v = torch.tensor(y_train_v)\n",
        "  x_val = torch.tensor(x_val.reshape((-1,28*28))).cuda()\n",
        "  y_val = torch.tensor(y_val)\n",
        "  x_test = torch.tensor(x_test.reshape((-1,28*28))).cuda()\n",
        "  y_test = torch.tensor(y_test)\n",
        "\n",
        "\n",
        "  #zadatak1(x_train, y_train, yoh_train, x_test, y_test)  \n",
        "  #zadatak2i3(x_train, y_train, yoh_train, x_test, y_test)\n",
        "  #train_mb(x_train_v, y_train_v, yoh_train, x_test, y_test, x_val, y_val, variable_learning_rate=True) # zadatak 4, 5 i 6\n",
        "  #zadatak8(x_test,y_test)\n",
        "  zadatak9(x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTvZu9RvVp1I",
        "outputId": "966e1cdc-cf11-4f7d-895d-c2d7fefdf96c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch -> Numpy done.\n",
            "Classifier created. Training...\n",
            "Training finished!\n",
            "Predictions made.\n",
            "\n",
            "Tocnost: 0.9404\n",
            "Odaziv i preciznost, redom za klase: [(0.976530612244898, 0.9522388059701492), (0.9885462555066079, 0.9697493517718236), (0.937015503875969, 0.9253588516746412), (0.9376237623762376, 0.9062200956937799), (0.9592668024439919, 0.9363817097415507), (0.9002242152466368, 0.9145785876993167), (0.9498956158663883, 0.9568874868559412), (0.9309338521400778, 0.9522388059701492), (0.9004106776180698, 0.936965811965812), (0.9137760158572844, 0.948559670781893)]\n",
            "Prosjecni odaziv i preciznost: [0.93942233 0.93991792]\n",
            "Matrica konfuzije:\n",
            "[[ 957    0    4    1    1    6    9    1    0    1]\n",
            " [   0 1122    3    2    0    1    2    1    4    0]\n",
            " [   8    6  967   11    3    3    7    8   17    2]\n",
            " [   4    3   16  947    1   16    0    9   12    2]\n",
            " [   1    1   10    1  942    2    4    2    3   16]\n",
            " [  10    4    3   36    6  803   13    1   14    2]\n",
            " [   9    2   13    1    5   16  910    1    1    0]\n",
            " [   1    8   21   10    8    1    0  957    3   19]\n",
            " [   8    4    6   25    7   26    6    7  877    8]\n",
            " [   7    7    2   11   33    4    0   18    5  922]]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}